import os
import re
import time
import requests
from typing import List, Tuple, Dict, Optional
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed
from google import genai
from google.genai import types

"""
full_pipeline.py

ë‹¨ì¼ ì‹¤í–‰ìœ¼ë¡œ ë‹¤ìŒì„ ìˆ˜í–‰:
1. source_mds ë‚´ ëª¨ë“  .md íŒŒì¼ ë°˜ë³µ
2. ê° íŒŒì¼ì—ì„œ '## references' ë˜ëŠ” '## reference' (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ) í—¤ë”© ì„¹ì…˜ë§Œ ì œê±° (ë‹¤ìŒ í—¤ë”© ì „ê¹Œì§€ë§Œ)
3. (ì°¸ì¡° ì œê±°ëœ) ì›ë³¸ìœ¼ë¡œë¶€í„° ë™ì‹œì— ë‹¤ìŒ 3 ì‘ì—… ë³‘ë ¬ ìˆ˜í–‰
    - ë²ˆì—­ (ê¸°ì¡´ parallel_translate.py ì™€ ìœ ì‚¬í•œ ì²­í¬ ë³‘ë ¬ ë²ˆì—­)
    - ë©”íƒ€ë°ì´í„° ìƒì„± (Gemini 2.5 Pro ëª¨ë¸, ì§€ì •ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
    - ì¸ìš©(MLA) ê²€ìƒ‰ (ê°€ì¥ ì²« ë²ˆì§¸ # í—¤ë”©ì„ ë…¼ë¬¸ ì œëª©ìœ¼ë¡œ ì‚¬ìš©, SerpAPI Google Scholar -> MLA)
4. ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ìµœì¢… Markdownìœ¼ë¡œ ê²°í•© í›„ translated_mds/<ì›ë³¸ì´ë¦„>_final.md ë¡œ ì €ì¥

ìµœì¢… ì¶œë ¥ í¬ë§·:
---
(YAML Front Matter ë©”íƒ€ë°ì´í„°)
---

> (MLA ì¸ìš©)

(ë²ˆì—­ ë³¸ë¬¸)

ì£¼ì˜: ê¸°ì¡´ ì½”ë“œ íŒŒì¼ ìˆ˜ì • ì—†ìŒ. ìƒˆë¡œìš´ íŒŒì¼ë§Œ ìƒì„±.
"""

# --------------------------------------------------
# í™˜ê²½ ë¡œë“œ
# --------------------------------------------------
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")  # ì¸ìš© ìƒì„±ì„ ìœ„í•œ SerpAPI í‚¤ (ì—†ìœ¼ë©´ ì¸ìš© ìƒëµ)

# ëª¨ë¸ ì´ë¦„
TRANSLATION_MODEL = "gemini-2.5-flash"
METADATA_MODEL = "gemini-2.5-pro"

SOURCE_DIR = "source_mds"
OUTPUT_DIR = "translated_mds"
PROMPT_EXAMPLE_DIR = "prompt_examples"
DEFAULT_MAX_CAP = 200
MAX_RETRIES = 2
RETRY_WAIT_SECONDS = 60

# --------------------------------------------------
# ê³µí†µ ìœ í‹¸
# --------------------------------------------------

def read_file(path: str) -> str:
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()


def write_file(path: str, text: str):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', encoding='utf-8') as f:
        f.write(text)


# --------------------------------------------------
# ë²ˆì—­ìš© ì˜ˆì‹œ ë¡œë“œ & ì‹œìŠ¤í…œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜ (parallel_translate.py ë¡œì§ ì°¨ìš©)
# --------------------------------------------------

def split_and_clean_example(text: str) -> List[str]:
    chunks = re.split(r'(^(?:#+ ).*$)', text, flags=re.MULTILINE)
    return [c.strip() for i, c in enumerate(chunks) if i % 2 == 0 and c.strip()]


def load_examples() -> Tuple[List[str], List[str]]:
    example_bases = ["small_example"]
    example_en_texts: List[str] = []
    example_ko_texts: List[str] = []
    for base in example_bases:
        en_path = os.path.join(PROMPT_EXAMPLE_DIR, f"{base}_en.md")
        ko_path = os.path.join(PROMPT_EXAMPLE_DIR, f"{base}_ko.md")
        if not (os.path.exists(en_path) and os.path.exists(ko_path)):
            raise FileNotFoundError(f"ì˜ˆì‹œ íŒŒì¼ ëˆ„ë½: {base}_en.md ë˜ëŠ” {base}_ko.md")
        en_pars = split_and_clean_example(read_file(en_path))
        ko_pars = split_and_clean_example(read_file(ko_path))
        for en_par, ko_par in zip(en_pars, ko_pars):
            example_en_texts.append(en_par)
            example_ko_texts.append(ko_par)
    if not example_en_texts:
        raise RuntimeError("ìœ íš¨í•œ ì˜ˆì‹œ í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return example_en_texts, example_ko_texts


def create_translation_system_instruction(example_en: List[str], example_ko: List[str]) -> str:
    header = """
You are a specialized translator for academic papers, translating Markdown documents from English to Korean.
Follow these rules meticulously:
1.  Translate the main content into professional, natural-sounding Korean.
2.  **Crucially, do not translate technical English terms.** Keep terms like 'Transformer', 'DETR', 'cross attention', 'encoder', 'decoder', etc., in their original English form.
3.  **Ensure that figure links are preserved.** Do not omit any figure URLs or accompanying descriptions.
4.  **Include references formatted as [1], but when encountering footnote markers like [^1], remove only the [^1] marker while keeping the adjacent explanation text intact.**
5.  **Use the provided examples as a reference for tone, translation style, and formatting.** This includes matching whitespace, line breaks, and paragraph divisions exactly as shown.
6.  **Do not create any new titles or headings; translate only the content youâ€™ve been given.**
7.  **Enhance readability by using bold formatting for important terms or key phrases, as in the examples.**

Now, here are multiple examples:
"""
    examples = []
    for en, ko in zip(example_en, example_ko):
        examples.append(f"""
---
### Translation Example ###
[English Original]
{en}

[Korean Translation]
{ko}
""")
    footer = """
---
Now, please translate the following English text into Korean based on all the rules and the examples provided.
"""
    return header + "".join(examples) + footer

# --------------------------------------------------
# ë©”íƒ€ë°ì´í„° ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
# --------------------------------------------------
METADATA_SYSTEM_PROMPT = """
ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë…¼ë¬¸ íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì— ì‚¬ìš©í•  ë©”íƒ€ë°ì´í„°ë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì¶œë ¥ í˜•ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

---
title: '<ê²Œì‹œê¸€ ì œëª©>'
description: '<ê²Œì‹œê¸€ ì„¤ëª…>'
date: '2025-08-01'
tags: ['<ê´€ë ¨ íƒœê·¸ 1>', '<ê´€ë ¨ íƒœê·¸ 2>']
excerpt: '<ê²Œì‹œê¸€ ì„¤ëª…>'
featuredImage: '<ê°€ì¥ í•µì‹¬ì ì¸ ì´ë¯¸ì§€ ì£¼ì†Œ>'
---

**ëª©í‘œ ë° ì—­í• :**

* ì‚¬ìš©ìê°€ ì œê³µí•œ ë§ˆí¬ë‹¤ìš´ ë…¼ë¬¸ íŒŒì¼ì—ì„œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì— í•„ìš”í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ìƒì„±í•©ë‹ˆë‹¤.
* ì œì‹œëœ í˜•ì‹ì— ë§ì¶° 'title', 'description', 'date', 'tags', 'excerpt', 'featuredImage' í•„ë“œë¥¼ ì •í™•í•˜ê²Œ ì±„ì›ë‹ˆë‹¤.
* 'title'ì€ ë…¼ë¬¸ì˜ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ë¸”ë¡œê·¸ ê²Œì‹œë¬¼ì— ì í•©í•˜ê²Œ ìƒì„±í•©ë‹ˆë‹¤.
* 'description'ì€ ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬, ì‚¬ìš©ìê°€ ë…¼ë¬¸ì„ ì½ê¸°ì „ì— descriptionì„ í†µí•´ ì „ì²´ì ì¸ ë‚´ìš©ì„ ë°”ë¡œ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ì—ëŠ” ë…¼ë¬¸ ì œëª©: <ë…¼ë¬¸ ì œëª©> í˜•ì‹ìœ¼ë¡œ ì˜ì–´ ì›ì–´ ê·¸ëŒ€ë¡œ ë…¼ë¬¸ ì œëª©ì„ ë§ë¶™ì—¬ì£¼ì„¸ìš”.
* 'date'ëŠ” í•­ìƒ '2025-08-01'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
* 'tags'ëŠ” ë…¼ë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë‚˜ ì£¼ì œë¥¼ íŒŒì•…í•˜ì—¬ 1~2ê°œì˜ ê´€ë ¨ íƒœê·¸ë¥¼ ë°°ì—´ í˜•ì‹ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. (ì˜ì–´ë¡œ ì‘ì„±)
* 'excerpt'ëŠ” 'description'ê³¼ ë™ì¼í•œ ë‚´ìš©ì„ ì‘ì„±í•©ë‹ˆë‹¤.
* 'featuredImage'ëŠ” ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë‚´ì˜ ì´ë¯¸ì§€ ì£¼ì†Œ ì¤‘ ë…¼ë¬¸ì˜ í•µì‹¬ ë‚´ìš©ì„ ê°€ì¥ ì˜ ëŒ€í‘œí•˜ê±°ë‚˜ ì‹œê°ì ìœ¼ë¡œ ì¤‘ìš”í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ í•´ë‹¹ URLì„ ì œê³µí•©ë‹ˆë‹¤. ë§Œì•½ ì ì ˆí•œ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ í•„ë“œë¥¼ ë¹„ì›Œë‘¡ë‹ˆë‹¤.
* AI ë…¼ë¬¸ì˜ ì „ë¬¸ì ì¸ ìš©ì–´(ì˜ˆ: Transformer, Contrastive loss, Cross attention ë“±)ëŠ” ì˜ì–´ ì›ì–´ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤. ê°€ëŠ¥í•œ, ìš©ì–´ëŠ” ì˜ì–´ì›ì–´ë¥¼ ìœ ì§€í•´ì£¼ì„¸ìš”.

**í–‰ë™ ë° ê·œì¹™:**

1. **ì…ë ¥ ì²˜ë¦¬:** ì‚¬ìš©ìê°€ ë§ˆí¬ë‹¤ìš´ ë…¼ë¬¸ íŒŒì¼ì„ ì…ë ¥í•˜ë©´, í•´ë‹¹ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
2. **ë©”íƒ€ë°ì´í„° ìƒì„±:** ì¶”ì¶œëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ„ì—ì„œ ì •ì˜ëœ í˜•ì‹ì— ë§ì¶° ê° ë©”íƒ€ë°ì´í„° í•„ë“œì˜ ë‚´ìš©ì„ ìƒì„±í•©ë‹ˆë‹¤.
* 'title': ë…¼ë¬¸ ì œëª©ì„ ì§ê´€ì ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. ë…ìë“¤ì´ ì œëª©ì„ ë³´ìë§ˆì ë¬´ìŠ¨ ë…¼ë¬¸ì¸ì§€ ì•Œ ìˆ˜ ìˆë„ë¡ ì‘ì„±í•©ë‹ˆë‹¤.
* 'description': ë…¼ë¬¸ì˜ í•µì‹¬ ìš”ì•½ì„ ì œê³µí•˜ì—¬ ë…ìì˜ í¥ë¯¸ë¥¼ ìœ ë°œí•©ë‹ˆë‹¤. (í•œê¸€ë¡œ ì ë˜, ì „ë¬¸ì ì¸ ì˜ì–´ ìš©ì–´ëŠ” ë²ˆì—­í•˜ì§€ ì•Šê³  ì›ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€, ë§ˆì§€ë§‰ ë¶€ë¶„ì˜ ë…¼ë¬¸ ì œëª©ì€ ì˜ì–´ ì›ì–´ ê·¸ëŒ€ë¡œ ìœ ì§€)
* 'tags': ë…¼ë¬¸ì˜ ì£¼ìš” ê°œë…, ì—°êµ¬ ë¶„ì•¼, ë°©ë²•ë¡  ë“±ì„ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ì„± ë†’ì€ íƒœê·¸ë¥¼ ì„ ì •í•©ë‹ˆë‹¤. (ì˜ì–´ë¡œ ì‘ì„±)
* 'excerpt': 'description'ê³¼ ë™ì¼í•œ ë‚´ìš©ì„ ì‘ì„±í•©ë‹ˆë‹¤.
* 'featuredImage': ë§ˆí¬ë‹¤ìš´ ë‚´ì˜ ëª¨ë“  ì´ë¯¸ì§€ URLì„ ê²€í† í•˜ì—¬, ë…¼ë¬¸ì˜ ì£¼ì œì™€ ê°€ì¥ ë°€ì ‘í•˜ê²Œ ê´€ë ¨ëœ ì‹œê° ìë£Œë¥¼ ëŒ€í‘œí•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì„ íƒí•©ë‹ˆë‹¤. ì´ í•„ë“œëŠ” ë°˜ë“œì‹œ ë”°ì˜´í‘œë¡œ ë¬¶ì¸ URL ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
3. **ì¶œë ¥ í˜•ì‹ ì¤€ìˆ˜:** ìƒì„±ëœ ë©”íƒ€ë°ì´í„°ë¥¼ ì •í™•íˆ ì œì‹œëœ YAML Front Matter í˜•ì‹(---ë¡œ ì‹œì‘í•˜ê³  ëë‚¨)ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤. ëª¨ë“  ë¬¸ìì—´ ê°’ì€ ì‘ì€ë”°ì˜´í‘œë¡œ ë¬¶ì–´ì•¼ í•©ë‹ˆë‹¤.
4. **ì‘ë‹µ ìƒì„¸í™”:** ë©”íƒ€ë°ì´í„° ìƒì„± ì‹œ, ê° í•„ë“œê°€ ì™œ ê·¸ë ‡ê²Œ ì±„ì›Œì¡ŒëŠ”ì§€ì— ëŒ€í•œ ê°„ëµí•œ ì„¤ëª…ì„ ì¶”ê°€í•  í•„ìš”ëŠ” ì—†ìœ¼ë©°, ì˜¤ì§ ìµœì¢… ë©”íƒ€ë°ì´í„° í˜•ì‹ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.

**ì¶œë ¥ ì˜ˆì‹œ:**

---
title: 'Flamingo: a Visual Language Model for Few-Shot Learning'
description: 'FlamingoëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ í˜¼í•©ëœ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, few-shot í•™ìŠµ í™˜ê²½ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” Visual Language Model (VLM)ì´ë‹¤. FlamingoëŠ” pretrainedëœ vision-only ë° language-only ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì—°ê²°í•˜ê³ , ì„ì˜ì˜ ìˆœì„œë¡œ interleavedëœ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. ì´ ëª¨ë¸ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì„ì¸ ëŒ€ê·œëª¨ ì›¹ ë°ì´í„°ë¡œ í•™ìŠµë˜ë©°, in-context few-shot í•™ìŠµ ëŠ¥ë ¥ì„ í†µí•´ ë‹¤ì–‘í•œ multimodal task (ì˜ˆ: visual question answering, image captioning ë“±)ì— ë¹ ë¥´ê²Œ ì ì‘í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ë…¼ë¬¸ ì œëª©: Flamingo: a Visual Language Model for Few-Shot Learning'
date: '2025-08-01'
tags: ['Visual Language Model', 'Few-shot Learning']
excerpt: 'FlamingoëŠ” ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ í˜¼í•©ëœ ì…ë ¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, few-shot í•™ìŠµ í™˜ê²½ì—ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” Visual Language Model (VLM)ì´ë‹¤. FlamingoëŠ” pretrainedëœ vision-only ë° language-only ëª¨ë¸ì„ íš¨ê³¼ì ìœ¼ë¡œ ì—°ê²°í•˜ê³ , ì„ì˜ì˜ ìˆœì„œë¡œ interleavedëœ ì´ë¯¸ì§€ ë° í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. ì´ ëª¨ë¸ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ê°€ ì„ì¸ ëŒ€ê·œëª¨ ì›¹ ë°ì´í„°ë¡œ í•™ìŠµë˜ë©°, in-context few-shot í•™ìŠµ ëŠ¥ë ¥ì„ í†µí•´ ë‹¤ì–‘í•œ multimodal task (ì˜ˆ: visual question answering, image captioning ë“±)ì— ë¹ ë¥´ê²Œ ì ì‘í•˜ëŠ” ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ë…¼ë¬¸ ì œëª©: Flamingo: a Visual Language Model for Few-Shot Learning'
featuredImage: 'https://cdn.mathpix.com/cropped/2025_07_26_7c316185968e7585aacbg-02.jpg?height=2242&width=1403&top_left_y=180&top_left_x=361'
---
"""

# --------------------------------------------------
# ë ˆí¼ëŸ°ìŠ¤ ì œê±°
# --------------------------------------------------

def remove_references_section(md: str) -> str:
    """'## references' ë˜ëŠ” '## reference' í—¤ë”©ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ë§Œ ì œê±°.

    - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ
    - í—¤ë”© ë¼ì¸ê³¼ ê·¸ *ë‹¤ìŒ ì²« ë²ˆì§¸ í—¤ë”©(ì„ì˜ì˜ #) ì§ì „ê¹Œì§€*ì˜ ë‚´ìš©ë§Œ ì œê±°
    - ë’¤ì— Appendix ë“± ë‹¤ë¥¸ í—¤ë”© ë¸”ë¡ì€ ë³´ì¡´
    - ì—¬ëŸ¬ ê°œ ì¡´ì¬í•˜ë©´ ëª¨ë‘ ì œê±° (ì•ˆì „ì„± ìœ„í•´ ë°˜ë³µ ì²˜ë¦¬)
    """
    lines = md.splitlines()
    out: List[str] = []
    i = 0
    ref_header_pattern = re.compile(r'^\s{0,3}#{2,6}\s*references?\s*$', re.IGNORECASE)
    any_header_pattern = re.compile(r'^\s{0,3}#+\s')
    while i < len(lines):
        line = lines[i]
        if ref_header_pattern.match(line):
            # Skip this references header line
            i += 1
            # Skip until next heading (any level)
            while i < len(lines) and not any_header_pattern.match(lines[i]):
                i += 1
            continue  # do not append skipped lines
        out.append(line)
        i += 1
    return "\n".join(out).rstrip() + "\n"

# --------------------------------------------------
# ì²« ë©”ì¸ ì œëª©(# ) ì¶”ì¶œ
# --------------------------------------------------

def extract_main_title(md: str) -> Optional[str]:
    for line in md.splitlines():
        if line.strip().startswith('# '):
            return line.strip()[2:].strip()
    return None

# --------------------------------------------------
# ì¸ìš© (SerpAPI Google Scholar -> MLA)
# --------------------------------------------------

def get_mla_citation(title: str) -> Optional[str]:
    """ì£¼ì–´ì§„ ë…¼ë¬¸ ì œëª©ìœ¼ë¡œ Google Scholar ê²€ìƒ‰ í›„ MLA ì¸ìš©ë¬¸ ë°˜í™˜.

    íë¦„:
      1) google_scholar ì—”ì§„ìœ¼ë¡œ ê²€ìƒ‰ â†’ ì²« organic result ì¶”ì¶œ
      2) result_id ê¸°ë°˜ google_scholar_cite í˜¸ì¶œ (ì—†ìœ¼ë©´ inline_links.serpapi_cite_link ì‚¬ìš©)
      3) citations ë°°ì—´ì—ì„œ title == 'MLA' ì¸ snippet ë°˜í™˜
    SERPAPI_API_KEY ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ None ë°˜í™˜.
    """
    if not SERPAPI_API_KEY:
        return None
    try:
        search_params = {
            "engine": "google_scholar",
            "q": title,
            "hl": "en",
            "api_key": SERPAPI_API_KEY,
        }
        r = requests.get("https://serpapi.com/search", params=search_params, timeout=30)
        r.raise_for_status()
        data = r.json()
        organic = data.get("organic_results", [])
        if not organic:
            return None
        top = organic[0]
        result_id = top.get("result_id")
        if result_id:
            cite_params = {
                "engine": "google_scholar_cite",
                "q": result_id,
                "hl": "en",
                "api_key": SERPAPI_API_KEY,
            }
            r2 = requests.get("https://serpapi.com/search", params=cite_params, timeout=30)
        else:
            cite_link = (top.get("inline_links") or {}).get("serpapi_cite_link")
            if not cite_link:
                return None
            connector = "&" if "?" in cite_link else "?"
            r2 = requests.get(f"{cite_link}{connector}api_key={SERPAPI_API_KEY}", timeout=30)
        r2.raise_for_status()
        cite_data = r2.json()
        for c in cite_data.get("citations", []):
            if (c.get("title") or "").strip().upper() == "MLA":
                snippet = (c.get("snippet") or "").strip()
                return snippet or None
        return None
    except Exception:
        return None

# --------------------------------------------------
# ë²ˆì—­ ê´€ë ¨ ë¶„í• 
# --------------------------------------------------

def split_markdown(content: str) -> List[str]:
    return re.split(r'(^(?:#+ ).*$)', content, flags=re.MULTILINE)


def needs_translation(i: int, chunk: str) -> bool:
    if i % 2 == 1:  # í—¤ë”
        return False
    return bool(chunk.strip())

# --------------------------------------------------
# ë²ˆì—­ ì²­í¬ í˜¸ì¶œ
# --------------------------------------------------

def translate_chunk(client: genai.Client, model_config: types.GenerateContentConfig, idx: int, text: str):
    attempt = 0
    while attempt <= MAX_RETRIES:
        try:
            resp = client.models.generate_content(
                model=TRANSLATION_MODEL,
                contents=text,
                config=model_config
            )
            return idx, "\n\n" + resp.text + "\n\n"
        except Exception as e:
            attempt += 1
            if attempt > MAX_RETRIES:
                print(f"   [ì‹¤íŒ¨] ì²­í¬ {idx} -> ì›ë¬¸ ìœ ì§€. ì—ëŸ¬: {e}")
                return idx, text
            print(f"   [ì¬ì‹œë„] ì²­í¬ {idx} ({attempt}/{MAX_RETRIES}) ì—ëŸ¬: {e} -> {RETRY_WAIT_SECONDS}s ëŒ€ê¸°")
            time.sleep(RETRY_WAIT_SECONDS)

# --------------------------------------------------
# íŒŒì¼ ë‹¨ìœ„ ë²ˆì—­ (ì²­í¬ ë™ì‹œ ì œì¶œ)
# --------------------------------------------------

def translate_content_all_at_once(md: str, client: genai.Client, model_config: types.GenerateContentConfig) -> str:
    chunks = split_markdown(md)
    targets = [(i, c) for i, c in enumerate(chunks) if needs_translation(i, c)]
    if not targets:
        return md
    max_workers = min(len(targets), DEFAULT_MAX_CAP) if targets else 1
    results: Dict[int, str] = {}
    submitted = 0
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = [ex.submit(translate_chunk, client, model_config, i, c) for i, c in targets]
        for fut in as_completed(futures):
            idx, translated = fut.result()
            results[idx] = translated
            submitted += 1
            if submitted % 5 == 0 or submitted == len(targets):
                print(f"      ë²ˆì—­ ì§„í–‰ë¥  {submitted}/{len(targets)} ({submitted/len(targets)*100:.1f}%)")
    final_parts: List[str] = []
    for i, original in enumerate(chunks):
        final_parts.append(results.get(i, original))
    return "".join(final_parts)

# --------------------------------------------------
# ë©”íƒ€ë°ì´í„° ìƒì„±
# --------------------------------------------------

def generate_metadata(client: genai.Client, markdown: str) -> str:
    try:
        resp = client.models.generate_content(
            model=METADATA_MODEL,
            contents=markdown + "\n\në©”íƒ€ë°ì´í„° ì‘ì„±í•´ì¤˜",
            config=types.GenerateContentConfig(
                temperature=1.0,
                system_instruction=METADATA_SYSTEM_PROMPT,
                thinking_config=types.ThinkingConfig(thinking_budget=-1)
            )
        )
        meta = resp.text.strip()
        # front matter ë³´ì •
        if '---' not in meta:
            meta = f"---\n{meta}\n---"
        else:
            # ì‹œì‘ê³¼ ëì´ --- ë¡œ ê°ì‹¸ì ¸ìˆëŠ”ì§€ í™•ì¸
            if not meta.startswith('---'):
                meta = '---\n' + meta
            if not re.search(r'\n---\s*$', meta):
                meta = meta.rstrip() + '\n---'
        return meta
    except Exception as e:
        print(f"[ë©”íƒ€ë°ì´í„° ì‹¤íŒ¨] {e}")
        return "---\ntitle: 'N/A'\ndescription: 'N/A'\ndate: '2025-08-01'\ntags: []\nexcerpt: 'N/A'\nfeaturedImage: ''\n---"

# --------------------------------------------------
# ê°œë³„ íŒŒì¼ ì²˜ë¦¬
# --------------------------------------------------

def process_file(path: str, translation_client: genai.Client, translation_config: types.GenerateContentConfig):
    name = os.path.basename(path)
    base, ext = os.path.splitext(name)
    print(f"\nğŸ“„ ì²˜ë¦¬ ì‹œì‘: {name}")
    original_md = read_file(path)
    cleaned_md = remove_references_section(original_md)
    main_title = extract_main_title(original_md) or base

    # ë³‘ë ¬: ë²ˆì—­ / ë©”íƒ€ë°ì´í„° / ì¸ìš©
    with ThreadPoolExecutor(max_workers=3) as ex:
        fut_translation = ex.submit(translate_content_all_at_once, cleaned_md, translation_client, translation_config)
        fut_metadata = ex.submit(generate_metadata, translation_client, cleaned_md)
        fut_citation = ex.submit(get_mla_citation, main_title)

        translated_text = fut_translation.result()
        metadata_text = fut_metadata.result()
        citation_text = fut_citation.result()

    if not citation_text:
        citation_text = f"Citation not found for title: {main_title}"

    final_output = f"{metadata_text}\n\n> {citation_text}\n{translated_text}".rstrip() + "\n"

    out_path = os.path.join(OUTPUT_DIR, f"{base}_final{ext}")
    write_file(out_path, final_output)
    print(f"âœ… ì™„ë£Œ: {out_path}")

# --------------------------------------------------
# ë©”ì¸
# --------------------------------------------------

def main():
    if not os.path.exists(SOURCE_DIR):
        raise FileNotFoundError(f"'{SOURCE_DIR}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")

    en_examples, ko_examples = load_examples()
    translation_system_instruction = create_translation_system_instruction(en_examples, ko_examples)

    translation_client = genai.Client(api_key=GOOGLE_API_KEY)
    translation_config = types.GenerateContentConfig(
        temperature=0,
        system_instruction=translation_system_instruction,
        thinking_config=types.ThinkingConfig(thinking_budget=0)
    )

    files = [f for f in os.listdir(SOURCE_DIR) if f.endswith('.md')]
    files.sort()
    if not files:
        print("ë²ˆì—­í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ì´ {len(files)}ê°œ íŒŒì¼ ì²˜ë¦¬. (ë²ˆì—­ ì²­í¬ ë™ì‹œ ìƒí•œ {DEFAULT_MAX_CAP})")

    for fname in files:
        process_file(os.path.join(SOURCE_DIR, fname), translation_client, translation_config)


if __name__ == '__main__':
    main()
